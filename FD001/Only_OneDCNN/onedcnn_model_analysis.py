import torch
import torch.nn as nn
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report, roc_curve, precision_recall_curve,
                             auc)
from torch.utils.data import Dataset, DataLoader
import time
import os
import warnings

warnings.filterwarnings('ignore')



class InferenceDataset(Dataset):
    """æ¨ç†æµ‹è¯•æ•°æ®é›†ç±»ï¼ˆé€‚é…åŸå§‹æ•°æ®ç”Ÿæˆå™¨æ ¼å¼ï¼‰"""

    def __init__(self, data_file, sequence_length=10, generator_id=1, normalize=True):
        """
        åˆå§‹åŒ–æ¨ç†æ•°æ®é›†

        Args:
            data_file: JSONæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆåŸå§‹æ•°æ®ç”Ÿæˆå™¨æ ¼å¼ï¼‰
            sequence_length: åºåˆ—é•¿åº¦ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
            generator_id: å‘ç”Ÿå™¨ID (1-5)ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå‘ç”Ÿå™¨
            normalize: æ˜¯å¦è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        """
        self.sequence_length = sequence_length
        self.generator_id = generator_id
        self.normalize = normalize

        # åŠ è½½æ¨ç†æ•°æ®
        print(f"åŠ è½½æ¨ç†æ•°æ®é›†: {data_file}")
        with open(data_file, 'r') as f:
            self.data = json.load(f)

        # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶è·å–æ•°æ®
        self.values, self.labels = self._load_data()

        # æ•°æ®é¢„å¤„ç†
        if normalize:
            self.values = self._normalize_data(self.values)

        # åˆ›å»ºåºåˆ—
        self.samples, self.sample_labels = self._create_sequences()

        print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬")
        print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(self.sample_labels):.3f}")

    def _load_data(self):
        """åŠ è½½æ•°æ®ï¼ˆé€‚é…åŸå§‹æ•°æ®ç”Ÿæˆå™¨æ ¼å¼ï¼‰"""
        # å°è¯•åŸå§‹æ•°æ®ç”Ÿæˆå™¨æ ¼å¼
        values_key = f"time_sequence_{self.generator_id}_value"
        labels_key = f"time_sequence_{self.generator_id}_label"

        if values_key in self.data and labels_key in self.data:
            print(f"ä½¿ç”¨æ•°æ®ç”Ÿæˆå™¨æ ¼å¼: {values_key}, {labels_key}")
            values = np.array(self.data[values_key], dtype=np.float32)
            labels = np.array(self.data[labels_key], dtype=np.float32)
        # å°è¯•ç®€å•æ ¼å¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
        elif 'values' in self.data and 'labels' in self.data:
            print("ä½¿ç”¨ç®€å•æ ¼å¼: values, labels")
            values = np.array(self.data['values'], dtype=np.float32)
            labels = np.array(self.data['labels'], dtype=np.float32)
        else:
            # å°è¯•å¯»æ‰¾å…¶ä»–å¯èƒ½çš„é”®
            available_keys = list(self.data.keys())
            print(f"å¯ç”¨æ•°æ®é”®: {available_keys}")

            # å°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼
            value_keys = [k for k in available_keys if 'value' in k.lower()]
            label_keys = [k for k in available_keys if 'label' in k.lower()]

            if value_keys and label_keys:
                print(f"è‡ªåŠ¨æ£€æµ‹åˆ°å€¼é”®: {value_keys[0]}, æ ‡ç­¾é”®: {label_keys[0]}")
                values = np.array(self.data[value_keys[0]], dtype=np.float32)
                labels = np.array(self.data[label_keys[0]], dtype=np.float32)
            else:
                raise ValueError(
                    f"æ•°æ®æ ¼å¼é”™è¯¯: æ— æ³•è¯†åˆ«æ•°æ®æ ¼å¼ã€‚æœŸæœ›çš„æ•°æ®ç”Ÿæˆå™¨æ ¼å¼åŒ…å« '{values_key}' å’Œ '{labels_key}' é”®")

        if len(values) != len(labels):
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸åŒ¹é…: values({len(values)}) != labels({len(labels)})")

        print(f"æ•°æ®ç»´åº¦: values={values.shape}, labels={labels.shape}")
        return values, labels

    def _normalize_data(self, values):
        """æ ‡å‡†åŒ–æ•°æ®"""
        mean = np.mean(values)
        std = np.std(values)
        normalized = (values - mean) / std
        print(f"æ•°æ®æ ‡å‡†åŒ–: mean={mean:.4f}, std={std:.4f}")
        return normalized

    def _create_sequences(self):
        """åˆ›å»ºåºåˆ—"""
        sequences = []
        sequence_labels = []

        # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦è¶³å¤Ÿ
        if len(self.values) < self.sequence_length:
            raise ValueError(f"æ•°æ®é•¿åº¦({len(self.values)})å°äºåºåˆ—é•¿åº¦({self.sequence_length})")

        # åˆ›å»ºæ»‘åŠ¨çª—å£
        for i in range(len(self.values) - self.sequence_length + 1):
            seq = self.values[i:i + self.sequence_length]
            label = self.labels[i + self.sequence_length - 1]

            sequences.append(seq)
            sequence_labels.append(label)

        return np.array(sequences, dtype=np.float32), np.array(sequence_labels, dtype=np.float32)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        label = self.sample_labels[idx]

        # è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œæ·»åŠ ç‰¹å¾ç»´åº¦
        sample_tensor = torch.FloatTensor(sample).unsqueeze(-1)  # å½¢çŠ¶: [sequence_length, 1]
        label_tensor = torch.FloatTensor([label])

        return sample_tensor, label_tensor


class ModelAnalyzer:
    """æ¨¡å‹æ¨ç†åˆ†æå™¨"""

    def __init__(self, model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = torch.device(device)
        self.model_path = model_path
        self.model = None
        self.config = None
        self.results = {}

        # åŠ è½½æ¨¡å‹
        self._load_model()

    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            print(f"åŠ è½½æ¨¡å‹: {self.model_path}")
            checkpoint = torch.load(self.model_path, map_location=self.device)

            # è·å–æ¨¡å‹é…ç½®
            if 'config' in checkpoint:
                self.config = checkpoint['config'].get('model_config', {})
            else:
                # é»˜è®¤é…ç½®ï¼ˆåº”ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                self.config = {
                    'input_dim': 1,
                    'seq_length': 10,
                    'conv_channels': 16,
                    'dropout_rate': 0.2
                }

            print(f"æ¨¡å‹é…ç½®: {self.config}")

            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            self.model = self._create_model()

            # åŠ è½½æƒé‡
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)

            self.model.to(self.device)
            self.model.eval()
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {self.device}")

        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹å®ä¾‹"""

        class Pure1DCNN(nn.Module):
            def __init__(self, input_dim=1, seq_length=10, conv_channels=16, dropout_rate=0.2):
                super(Pure1DCNN, self).__init__()
                self.conv1d = nn.Conv1d(input_dim, conv_channels, kernel_size=3, padding=1)
                self.batchnorm = nn.BatchNorm1d(conv_channels)
                self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
                self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
                self.dropout = nn.Dropout(dropout_rate)
                self.fc = nn.Linear(conv_channels, 1)

            def forward(self, x):
                x = x.transpose(1, 2)
                x = self.conv1d(x)
                x = torch.relu(x)
                x = self.batchnorm(x)
                x = self.pool(x)
                x = self.global_avg_pool(x)
                x = x.squeeze(-1)
                x = self.dropout(x)
                x = self.fc(x)
                return torch.sigmoid(x)

        return Pure1DCNN(**self.config)

    def inference(self, data_loader, threshold=0.5):
        """æ‰§è¡Œæ¨ç†"""
        all_predictions = []
        all_probabilities = []
        all_labels = []
        inference_times = []

        print("å¼€å§‹æ¨ç†...")

        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(data_loader):
                data, target = data.to(self.device), target.to(self.device)

                # è®¡æ—¶æ¨ç†
                start_time = time.perf_counter()
                output = self.model(data)
                inference_time = time.perf_counter() - start_time

                probabilities = output.cpu().numpy().flatten()
                predictions = (probabilities > threshold).astype(int)

                all_probabilities.extend(probabilities)
                all_predictions.extend(predictions)
                all_labels.extend(target.cpu().numpy().flatten())
                inference_times.append(inference_time)

                if (batch_idx + 1) % 10 == 0:
                    print(f"å·²å¤„ç† {batch_idx + 1}/{len(data_loader)} æ‰¹æ¬¡")

        total_samples = len(all_labels)
        total_time = sum(inference_times)

        results = {
            'predictions': np.array(all_predictions),
            'probabilities': np.array(all_probabilities),
            'labels': np.array(all_labels),
            'inference_times': inference_times,
            'total_samples': total_samples,
            'total_time': total_time,
            'avg_time_per_sample': total_time / total_samples * 1000,  # æ¯«ç§’
            'throughput': total_samples / total_time  # æ ·æœ¬/ç§’
        }

        print(f"æ¨ç†å®Œæˆ: {total_samples} ä¸ªæ ·æœ¬, æ€»æ—¶é—´: {total_time:.2f} ç§’")
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {results['avg_time_per_sample']:.2f} ms/æ ·æœ¬")
        print(f"ååé‡: {results['throughput']:.2f} æ ·æœ¬/ç§’")

        return results

    def calculate_metrics(self, predictions, labels, probabilities):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        # åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(labels, predictions)
        precision = precision_score(labels, predictions, zero_division=0)
        recall = recall_score(labels, predictions, zero_division=0)
        f1 = f1_score(labels, predictions, zero_division=0)

        # AUC
        if len(np.unique(labels)) > 1:
            try:
                auc_score = roc_auc_score(labels, probabilities)
            except:
                auc_score = 0.0
        else:
            auc_score = 0.0

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(labels, predictions)
        if cm.size == 4:
            tn, fp, fn, tp = cm.ravel()
        else:
            tn, fp, fn, tp = 0, 0, 0, 0

        # è¯¦ç»†æŒ‡æ ‡
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # é˜´æ€§é¢„æµ‹å€¼
        prevalence = (tp + fn) / (tp + fp + tn + fn)  # æ‚£ç—…ç‡

        # F1åˆ†æ•°çš„å˜ä½“
        f2_score_value = (5 * precision * recall) / (4 * precision + recall + 1e-8) if (precision + recall) > 0 else 0

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'f2_score': f2_score_value,
            'auc': auc_score,
            'specificity': specificity,
            'fpr': fpr,
            'fnr': fnr,
            'npv': npv,
            'prevalence': prevalence,
            'confusion_matrix': cm.tolist(),
            'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)
        }

    def analyze_threshold(self, probabilities, labels, thresholds=None):
        """åˆ†æä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½"""
        if thresholds is None:
            thresholds = np.arange(0.1, 1.0, 0.05)

        threshold_analysis = {}

        for threshold in thresholds:
            predictions = (probabilities > threshold).astype(int)
            metrics = self.calculate_metrics(predictions, labels, probabilities)
            threshold_analysis[threshold] = metrics

        return threshold_analysis

    def find_optimal_threshold(self, probabilities, labels, metric='f1'):
        """å¯»æ‰¾æœ€ä½³é˜ˆå€¼"""
        thresholds = np.arange(0.01, 1.0, 0.01)
        best_threshold = 0.5
        best_metric_value = 0

        for threshold in thresholds:
            predictions = (probabilities > threshold).astype(int)
            metrics = self.calculate_metrics(predictions, labels, probabilities)

            metric_value = metrics.get(metric, 0)
            if metric_value > best_metric_value:
                best_metric_value = metric_value
                best_threshold = threshold

        return best_threshold, best_metric_value

    def plot_confusion_matrix(self, cm, title="æ··æ·†çŸ©é˜µ", save_path=None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['é¢„æµ‹æ­£å¸¸', 'é¢„æµ‹æ•…éšœ'],
                    yticklabels=['çœŸå®æ­£å¸¸', 'çœŸå®æ•…éšœ'])
        plt.title(title, fontsize=14)
        plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
        plt.show()

    def plot_roc_curve(self, labels, probabilities, title="ROCæ›²çº¿", save_path=None):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if len(np.unique(labels)) > 1:
            fpr, tpr, thresholds = roc_curve(labels, probabilities)
            auc_score = roc_auc_score(labels, probabilities)

            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, label=f'ROCæ›²çº¿ (AUC = {auc_score:.4f})', linewidth=2)
            plt.plot([0, 1], [0, 1], 'k--', label='éšæœºåˆ†ç±»å™¨', linewidth=1, alpha=0.5)
            plt.fill_between(fpr, tpr, alpha=0.2)
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('å‡æ­£ç‡ (FPR)', fontsize=12)
            plt.ylabel('çœŸæ­£ç‡ (TPR)', fontsize=12)
            plt.title(title, fontsize=14)
            plt.legend(loc='lower right')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ROCæ›²çº¿å·²ä¿å­˜: {save_path}")
            plt.show()

            return auc_score, fpr, tpr, thresholds
        else:
            print("è­¦å‘Š: æ•°æ®ä¸­åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•ç»˜åˆ¶ROCæ›²çº¿")
            return 0.0, None, None, None

    def plot_precision_recall_curve(self, labels, probabilities, title="ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿", save_path=None):
        """ç»˜åˆ¶ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿"""
        precision, recall, thresholds = precision_recall_curve(labels, probabilities)

        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, label='P-Ræ›²çº¿', linewidth=2)
        plt.fill_between(recall, precision, alpha=0.2)
        plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12)
        plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
        plt.title(title, fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"P-Ræ›²çº¿å·²ä¿å­˜: {save_path}")
        plt.show()

        return precision, recall, thresholds

    def plot_threshold_analysis(self, threshold_analysis, save_path=None):
        """ç»˜åˆ¶é˜ˆå€¼åˆ†æå›¾"""
        thresholds = list(threshold_analysis.keys())
        f1_scores = [threshold_analysis[t]['f1_score'] for t in thresholds]
        precisions = [threshold_analysis[t]['precision'] for t in thresholds]
        recalls = [threshold_analysis[t]['recall'] for t in thresholds]

        plt.figure(figsize=(10, 6))
        plt.plot(thresholds, f1_scores, 'b-', label='F1åˆ†æ•°', linewidth=2)
        plt.plot(thresholds, precisions, 'r-', label='ç²¾ç¡®ç‡', linewidth=2)
        plt.plot(thresholds, recalls, 'g-', label='å¬å›ç‡', linewidth=2)
        plt.xlabel('åˆ†ç±»é˜ˆå€¼', fontsize=12)
        plt.ylabel('åˆ†æ•°', fontsize=12)
        plt.title('é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ', fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"é˜ˆå€¼åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        plt.show()

        # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
        best_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_idx]
        best_f1 = f1_scores[best_idx]

        print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.2f} (F1åˆ†æ•°: {best_f1:.4f})")
        return best_threshold

    def plot_probability_distribution(self, probabilities, labels, title="é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ", save_path=None):
        """ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"""
        pred_normal = probabilities[labels == 0]
        pred_fault = probabilities[labels == 1]

        plt.figure(figsize=(10, 6))

        if len(pred_normal) > 0:
            plt.hist(pred_normal, bins=20, alpha=0.7, label='æ­£å¸¸æ ·æœ¬', color='green')
        if len(pred_fault) > 0:
            plt.hist(pred_fault, bins=20, alpha=0.7, label='æ•…éšœæ ·æœ¬', color='red')

        plt.axvline(x=0.5, color='blue', linestyle='--', label='é˜ˆå€¼(0.5)')
        plt.xlabel('é¢„æµ‹æ¦‚ç‡', fontsize=12)
        plt.ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
        plt.title(title, fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
        plt.show()

    def generate_detailed_report(self, results, dataset_name="æ¨ç†æ•°æ®é›†"):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        metrics = self.calculate_metrics(
            results['predictions'],
            results['labels'],
            results['probabilities']
        )

        print(f"\n{'=' * 80}")
        print(f"ä»…1D-CNNæ¨¡å‹æ¨ç†åˆ†ææŠ¥å‘Š - {dataset_name}")
        print(f"{'=' * 80}")

        # æ•°æ®é›†ç»Ÿè®¡
        print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {results['total_samples']}")
        print(f"  æ­£æ ·æœ¬æ•°: {np.sum(results['labels'])}")
        print(f"  è´Ÿæ ·æœ¬æ•°: {len(results['labels']) - np.sum(results['labels'])}")
        print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(results['labels']):.3f}")

        # æ€§èƒ½æŒ‡æ ‡
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ (é˜ˆå€¼=0.5):")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.4f}")
        print(f"  å¬å›ç‡ (Recall): {metrics['recall']:.4f}")
        print(f"  F1åˆ†æ•°: {metrics['f1_score']:.4f}")
        print(f"  F2åˆ†æ•°: {metrics['f2_score']:.4f}")
        print(f"  AUC: {metrics['auc']:.4f}")
        print(f"  ç‰¹å¼‚åº¦ (Specificity): {metrics['specificity']:.4f}")
        print(f"  é˜´æ€§é¢„æµ‹å€¼ (NPV): {metrics['npv']:.4f}")

        # é”™è¯¯ç‡
        print(f"\nâš ï¸  é”™è¯¯åˆ†æ:")
        print(f"  å‡æ­£ç‡ (FPR): {metrics['fpr']:.4f}")
        print(f"  å‡è´Ÿç‡ (FNR): {metrics['fnr']:.4f}")

        # æ¨ç†æ€§èƒ½
        print(f"\nâš¡ æ¨ç†æ€§èƒ½:")
        print(f"  æ€»æ¨ç†æ—¶é—´: {results['total_time']:.2f} ç§’")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {results['avg_time_per_sample']:.2f} æ¯«ç§’/æ ·æœ¬")
        print(f"  ååé‡: {results['throughput']:.2f} æ ·æœ¬/ç§’")

        # æ··æ·†çŸ©é˜µ
        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        cm = np.array(metrics['confusion_matrix'])
        print(f"         é¢„æµ‹æ­£å¸¸  é¢„æµ‹æ•…éšœ")
        print(f"çœŸå®æ­£å¸¸   {cm[0][0]:6d}    {cm[0][1]:6d}")
        print(f"çœŸå®æ•…éšœ   {cm[1][0]:6d}    {cm[1][1]:6d}")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(results['labels'], results['predictions'],
                                    target_names=['æ­£å¸¸', 'æ•…éšœ'], digits=4))

        # ä¿å­˜ç»“æœ
        report_data = {
            'dataset_name': dataset_name,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'model_path': self.model_path,
            'model_config': self.config,
            'dataset_statistics': {
                'total_samples': int(results['total_samples']),
                'positive_samples': int(np.sum(results['labels'])),
                'negative_samples': int(len(results['labels']) - np.sum(results['labels'])),
                'positive_ratio': float(np.mean(results['labels']))
            },
            'performance_metrics': metrics,
            'inference_performance': {
                'total_time': float(results['total_time']),
                'avg_time_per_sample': float(results['avg_time_per_sample']),
                'throughput': float(results['throughput'])
            },
            'predictions': results['predictions'].tolist(),
            'probabilities': results['probabilities'].tolist(),
            'labels': results['labels'].tolist()
        }

        return report_data, metrics

    def run_analysis(self, data_file, batch_size=64, threshold=0.5,
                     save_plots=True, output_dir='results', generator_id=1):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if save_plots and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # åŠ è½½æ¨ç†æ•°æ®é›†
        dataset = InferenceDataset(data_file,
                                   sequence_length=self.config['seq_length'],
                                   generator_id=generator_id)
        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

        print(f"\n{'=' * 80}")
        print("å¼€å§‹æ¨ç†æµ‹è¯•åˆ†æ")
        print(f"æ•°æ®é›†: {data_file}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"åˆ†ç±»é˜ˆå€¼: {threshold}")
        print(f"å‘ç”Ÿå™¨ID: {generator_id}")
        print(f"{'=' * 80}")

        # æ‰§è¡Œæ¨ç†
        results = self.inference(data_loader, threshold=threshold)

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_data, metrics = self.generate_detailed_report(results, "æ¨ç†æµ‹è¯•é›†")

        # å¯è§†åŒ–åˆ†æ
        if save_plots:
            print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

            # æ··æ·†çŸ©é˜µ
            cm = np.array(metrics['confusion_matrix'])
            self.plot_confusion_matrix(
                cm,
                title="ä»…1D-CNNæ¨¡å‹æ··æ·†çŸ©é˜µ",
                save_path=os.path.join(output_dir, 'confusion_matrix.png')
            )

            # ROCæ›²çº¿
            if len(np.unique(results['labels'])) > 1:
                self.plot_roc_curve(
                    results['labels'], results['probabilities'],
                    title="ä»…1D-CNNæ¨¡å‹ROCæ›²çº¿",
                    save_path=os.path.join(output_dir, 'roc_curve.png')
                )

            # ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
            self.plot_precision_recall_curve(
                results['labels'], results['probabilities'],
                title="ä»…1D-CNNæ¨¡å‹ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿",
                save_path=os.path.join(output_dir, 'pr_curve.png')
            )

            # æ¦‚ç‡åˆ†å¸ƒ
            self.plot_probability_distribution(
                results['probabilities'], results['labels'],
                title="ä»…1D-CNNæ¨¡å‹é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ",
                save_path=os.path.join(output_dir, 'probability_distribution.png')
            )

            # é˜ˆå€¼åˆ†æ
            threshold_analysis = self.analyze_threshold(
                results['probabilities'], results['labels']
            )
            best_threshold = self.plot_threshold_analysis(
                threshold_analysis,
                save_path=os.path.join(output_dir, 'threshold_analysis.png')
            )

            report_data['optimal_threshold'] = float(best_threshold)
            report_data['generator_id'] = generator_id

        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(output_dir, 'inference_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… åˆ†æå®Œæˆ!")
        print(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        if save_plots:
            print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}/")

        return report_data, results


def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œæ¨ç†åˆ†æ"""

    # é…ç½®å‚æ•°
    MODEL_PATH = "pure_1dcnn_best.pth"  # è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
    INFERENCE_DATA_FILE = "test.json"  # æ¨ç†æ•°æ®é›†è·¯å¾„
    BATCH_SIZE = 64  # æ‰¹æ¬¡å¤§å°
    THRESHOLD = 0.5  # åˆ†ç±»é˜ˆå€¼
    GENERATOR_ID = 1  # å‘ç”Ÿå™¨ID
    SAVE_PLOTS = True  # æ˜¯å¦ä¿å­˜å›¾è¡¨
    OUTPUT_DIR = "inference_results"  # è¾“å‡ºç›®å½•

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ {MODEL_PATH} ä¸å­˜åœ¨")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„")
        return

    if not os.path.exists(INFERENCE_DATA_FILE):
        print(f"âŒ é”™è¯¯: æ¨ç†æ•°æ®æ–‡ä»¶ {INFERENCE_DATA_FILE} ä¸å­˜åœ¨")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„")
        return

    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = ModelAnalyzer(MODEL_PATH)

        # è¿è¡Œå®Œæ•´åˆ†æ
        report, results = analyzer.run_analysis(
            data_file=INFERENCE_DATA_FILE,
            batch_size=BATCH_SIZE,
            threshold=THRESHOLD,
            save_plots=SAVE_PLOTS,
            output_dir=OUTPUT_DIR,
            generator_id=GENERATOR_ID
        )

        # æ‰“å°æœ€ä½³é˜ˆå€¼å»ºè®®
        best_threshold, best_f1 = analyzer.find_optimal_threshold(
            results['probabilities'], results['labels'], metric='f1'
        )

        print(f"\nğŸ’¡ æœ€ä½³é˜ˆå€¼å»ºè®®:")
        print(f"  åŸºäºF1åˆ†æ•°çš„æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}")
        print(f"  å¯¹åº”çš„F1åˆ†æ•°: {best_f1:.4f}")

        # ä½¿ç”¨æœ€ä½³é˜ˆå€¼é‡æ–°è¯„ä¼°
        if best_threshold != THRESHOLD:
            print(f"\nä½¿ç”¨æœ€ä½³é˜ˆå€¼ {best_threshold:.3f} é‡æ–°è¯„ä¼°:")
            predictions_best = (results['probabilities'] > best_threshold).astype(int)
            metrics_best = analyzer.calculate_metrics(
                predictions_best, results['labels'], results['probabilities']
            )
            print(
                f"  F1åˆ†æ•°: {metrics_best['f1_score']:.4f} (åŸå§‹é˜ˆå€¼: {report['performance_metrics']['f1_score']:.4f})")
            print(
                f"  ç²¾ç¡®ç‡: {metrics_best['precision']:.4f} (åŸå§‹é˜ˆå€¼: {report['performance_metrics']['precision']:.4f})")
            print(f"  å¬å›ç‡: {metrics_best['recall']:.4f} (åŸå§‹é˜ˆå€¼: {report['performance_metrics']['recall']:.4f})")

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def test_dataset_format():
    """æµ‹è¯•æ•°æ®é›†æ ¼å¼è¯†åˆ«åŠŸèƒ½"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = {
        "time_sequence_1_value": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0],
        "time_sequence_1_label": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        "time_sequence_2_value": [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0],
        "time_sequence_2_label": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
    }

    # ä¿å­˜æµ‹è¯•æ•°æ®
    test_file = "test_dataset.json"
    with open(test_file, 'w') as f:
        json.dump(test_data, f)

    print(f"åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {test_file}")

    try:
        # æµ‹è¯•æ•°æ®åŠ è½½
        dataset = InferenceDataset(test_file, sequence_length=10, generator_id=1)
        print(f"æµ‹è¯•æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")

        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        data_loader = DataLoader(dataset, batch_size=2, shuffle=False)
        print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ: {len(data_loader)} ä¸ªæ‰¹æ¬¡")

        # æµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        for batch_idx, (data, target) in enumerate(data_loader):
            print(f"æ‰¹æ¬¡ {batch_idx}: æ•°æ®å½¢çŠ¶={data.shape}, æ ‡ç­¾å½¢çŠ¶={target.shape}")
            if batch_idx >= 1:
                break

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_file)
        print(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶: {test_file}")

    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        if os.path.exists(test_file):
            os.remove(test_file)


if __name__ == "__main__":
    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼ˆå¯é€‰ï¼‰
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    except:
        print("è­¦å‘Š: å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")

    # æµ‹è¯•æ•°æ®é›†æ ¼å¼è¯†åˆ«
    print("æµ‹è¯•æ•°æ®é›†æ ¼å¼è¯†åˆ«...")
    test_dataset_format()

    print("\n" + "=" * 80)
    print("å¼€å§‹æ­£å¼æ¨ç†åˆ†æ")
    print("=" * 80)

    # è¿è¡Œä¸»å‡½æ•°
    main()